{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf4jI56Zluhe"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 kaggle.json\n",
        "!kaggle datasets download  'kazanova/sentiment140'\n",
        "!unzip sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "tM_QHXkb-LLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import bz2\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification"
      ],
      "metadata": {
        "id": "MTlPAmneqGBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text=text.lower()\n",
        "    text= re.sub(r'(.)1+', r'1', text) #REPEATING CHARS\n",
        "    text=re.sub('((www.[^s]+)|(https?://[^s]+))',' ',text) #URLS\n",
        "    text=re.sub('[0-9]+', '', text) #NUMBERS\n",
        "    text=\" \".join(filter(lambda x:x[0]!='@', text.split())) #REPLY\n",
        "    return text"
      ],
      "metadata": {
        "id": "ufhYLoAXq33E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('./training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1',names=['label','ids','date','flag','user','text']).sample(frac = 1).reset_index(drop=True)\n",
        "df['text']=df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "GwI1yo2CopEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=df.iloc[:200000]\n",
        "test_data=df.iloc[200000:250000]\n",
        "del df"
      ],
      "metadata": {
        "id": "fNdbtj4FkldB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER..."
      ],
      "metadata": {
        "id": "cDLNGcVM_br7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=64\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = max_length)"
      ],
      "metadata": {
        "id": "wyRf-10D-FAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row=self.df.iloc[idx]\n",
        "        input_ids=torch.tensor(tokenizer.encode(row['text'],padding='max_length',max_length=max_length,truncation=True))\n",
        "        attention_mask=torch.where(input_ids!=1,1,0)\n",
        "        return {'input_ids':input_ids,\n",
        "         'attention_mask': attention_mask,\n",
        "         'label':torch.tensor(0 if row['label']==0 else 1)}"
      ],
      "metadata": {
        "id": "hYO878J__aCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_p=SentimentDataset(train_data)\n",
        "test_p=SentimentDataset(test_data)"
      ],
      "metadata": {
        "id": "LmzmNAe_BGyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QIW8YQLs-ql-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_p,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(test_p,batch_size=32)"
      ],
      "metadata": {
        "id": "bKrHmZIvCORi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mini_batch(samples):\n",
        "    input_ids = [s['input_ids'] for s in samples]\n",
        "    attention_mask = [(s['attention_mask']) for s in samples]\n",
        "    label = [s['label'] for s in samples]\n",
        "    l=max_length\n",
        "    input_ids=torch.stack(input_ids)[:,:l]\n",
        "    attention_mask=torch.stack(attention_mask)[:,:l]\n",
        "    label=torch.stack(label)\n",
        "    return input_ids, attention_mask, label"
      ],
      "metadata": {
        "id": "FoLTDqvh-1EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs=30):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        corrects=0.0\n",
        "        total=0.0\n",
        "        for batch in tqdm.tqdm(train_loader):\n",
        "            input_ids=batch['input_ids']\n",
        "            attention_mask=batch['attention_mask']\n",
        "            labels=batch['label']\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids=batch['input_ids']\n",
        "                attention_mask=batch['attention_mask']\n",
        "                labels=batch['label']\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                corrects+=torch.sum(outputs.logits.argmax(dim=-1)==labels).item()\n",
        "                total+=outputs.logits.size(0)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(test_loader)\n",
        "\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), \"best_model_roberta.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f},Valid acc: {corrects/total}\")"
      ],
      "metadata": {
        "id": "9pbvo_pzCTal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "PgsCkoVBCmqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(input_):\n",
        "  input_ids=torch.tensor([tokenizer.encode(input_,padding='max_length',max_length=max_length,truncation=True)]).to(device)\n",
        "  outputs=model(input_ids)[0].argmax(dim=-1)\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "_1_49PAYCoff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJR3djmek9TP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}