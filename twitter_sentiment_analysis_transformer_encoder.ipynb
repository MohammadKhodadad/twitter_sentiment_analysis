{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iJkTiIBhSgy"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 kaggle.json\n",
        "!kaggle datasets download  'kazanova/sentiment140'\n",
        "!unzip sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PubBiCCCOzRM"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWGyI_UwO1TA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import bz2\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrtn-B2iO2He"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(.)1+', r'1', text)  # REPEATING CHARS\n",
        "    text = re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', text)  # URLS\n",
        "    text = re.sub('[0-9]+', '', text)  # NUMBERS\n",
        "    text = \" \".join(filter(lambda x: x[0] != '@', text.split()))  # REPLY\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9-AUV6RHV_m"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1',\n",
        "                 names=['label', 'ids', 'date', 'flag', 'user', 'text']).sample(frac=1).reset_index(drop=True)\n",
        "df['text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKDh_-rnO46R"
      },
      "outputs": [],
      "source": [
        "train_data = df.iloc[:200000]\n",
        "test_data = df.iloc[200000:250000]\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvaS9VMoO6Py"
      },
      "outputs": [],
      "source": [
        "max_length = 64\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
        "    'roberta-base', max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OagpNkyO7uj"
      },
      "outputs": [],
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        input_ids = torch.tensor(tokenizer.encode(\n",
        "            row['text'], padding='max_length', max_length=max_length, truncation=True))\n",
        "        attention_mask = torch.where(input_ids != 1, False, True)\n",
        "        return {'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'label': torch.tensor(0.0 if row['label'] == 0 else 1.0)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IOJcJAfO8-I"
      },
      "outputs": [],
      "source": [
        "train_p = SentimentDataset(train_data)\n",
        "test_p = SentimentDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0ezfCDRPe4i"
      },
      "outputs": [],
      "source": [
        "class Sentiment_Model(torch.nn.Module):\n",
        "    def __init__(self, embed_dim=64, max_seq_len=max_length):\n",
        "        super(Sentiment_Model, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(len(tokenizer), embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "        self.mha1 = nn.MultiheadAttention(\n",
        "            embed_dim, 4, 0.2, kdim=embed_dim, vdim=embed_dim)\n",
        "        self.dense = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        word_embeddings = self.word_embedding(input_ids)\n",
        "        positional_embeddings = self.pos_embedding(\n",
        "            torch.arange(input_ids.size(1)).to(device))\n",
        "\n",
        "        input_embeddings = word_embeddings + positional_embeddings\n",
        "\n",
        "        attn_output1, attn_output_weights = self.mha1(\n",
        "            input_embeddings, input_embeddings, input_embeddings)\n",
        "        # attn_output2, attn_output_weights = self.mha2(attn_output1, attn_output1, attn_output1)\n",
        "        mean_output = attn_output1.mean(dim=1)\n",
        "\n",
        "        outputs = self.dense(mean_output)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NODVEtVAO-sy"
      },
      "outputs": [],
      "source": [
        "model = Sentiment_Model()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device='cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_rKez3FPC7W"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_p, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_p, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DOKzmSbPG2S"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs=30):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0.0\n",
        "        train_total = 0.0\n",
        "        test_corrects = 0.0\n",
        "        test_total = 0.0\n",
        "        for batch in tqdm.tqdm(train_loader):\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask'].T\n",
        "            labels = batch['label']\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids).view(-1,)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "            train_corrects += torch.sum((outputs >\n",
        "                                        0.5).float() == labels).item()\n",
        "            train_total += outputs.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids']\n",
        "                attention_mask = batch['attention_mask'].T\n",
        "                labels = batch['label']\n",
        "\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(input_ids).view(-1,)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_corrects += torch.sum((outputs >\n",
        "                                           0.5).float() == labels).item()\n",
        "                test_total += outputs.size(0)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(test_loader)\n",
        "\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f},Train acc: {train_corrects/train_total}, Valid Loss: {avg_valid_loss:.4f},Valid acc: {test_corrects/test_total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9D4rae6PIbW"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-QPOLTyPJjI"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(input_):\n",
        "    input_ids = torch.tensor([tokenizer.encode(\n",
        "        input_, padding='max_length', max_length=max_length, truncation=True)]).to(device)\n",
        "    # print(input_ids)\n",
        "    outputs = model(input_ids)[0].argmax(dim=-1)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3qELJSihRWu"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkM7wxVSZlE4"
      },
      "outputs": [],
      "source": [
        "print(run_pipeline('I feel so good'))\n",
        "print(run_pipeline('I lost my mother today. I miss her. I wish I could have her back'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
