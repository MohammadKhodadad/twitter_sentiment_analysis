{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf4jI56Zluhe"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 kaggle.json\n",
        "!kaggle datasets download  'kazanova/sentiment140'\n",
        "!unzip sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM_QHXkb-LLa"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTlPAmneqGBG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import bz2\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufhYLoAXq33E"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(.)1+', r'1', text)  # REPEATING CHARS\n",
        "    text = re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', text)  # URLS\n",
        "    text = re.sub('[0-9]+', '', text)  # NUMBERS\n",
        "    text = \" \".join(filter(lambda x: x[0] != '@', text.split()))  # REPLY\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwI1yo2CopEO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1',\n",
        "                 names=['label', 'ids', 'date', 'flag', 'user', 'text']).sample(frac=1).reset_index(drop=True)\n",
        "df['text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNdbtj4FkldB"
      },
      "outputs": [],
      "source": [
        "train_data = df.iloc[:200000]\n",
        "test_data = df.iloc[200000:250000]\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDLNGcVM_br7"
      },
      "source": [
        "# TOKENIZER..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyRf-10D-FAV"
      },
      "outputs": [],
      "source": [
        "max_length = 64\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
        "    'roberta-base', max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYO878J__aCM"
      },
      "outputs": [],
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        input_ids = torch.tensor(tokenizer.encode(\n",
        "            row['text'], padding='max_length', max_length=max_length, truncation=True))\n",
        "        attention_mask = torch.where(input_ids != 1, 1, 0)\n",
        "        return {'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'label': torch.tensor(0 if row['label'] == 0 else 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmzmNAe_BGyj"
      },
      "outputs": [],
      "source": [
        "train_p = SentimentDataset(train_data)\n",
        "test_p = SentimentDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIW8YQLs-ql-"
      },
      "outputs": [],
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKrHmZIvCORi"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_p, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_p, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pbvo_pzCTal"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs=30):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        corrects = 0.0\n",
        "        total = 0.0\n",
        "        for batch in tqdm.tqdm(train_loader):\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label']\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids']\n",
        "                attention_mask = batch['attention_mask']\n",
        "                labels = batch['label']\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(input_ids=input_ids,\n",
        "                                attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                corrects += torch.sum(outputs.logits.argmax(dim=-1)\n",
        "                                      == labels).item()\n",
        "                total += outputs.logits.size(0)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(test_loader)\n",
        "\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), \"best_model_roberta.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f},Valid acc: {corrects/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgsCkoVBCmqZ"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1_49PAYCoff"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(input_):\n",
        "    input_ids = torch.tensor([tokenizer.encode(\n",
        "        input_, padding='max_length', max_length=max_length, truncation=True)]).to(device)\n",
        "    outputs = model(input_ids)[0].argmax(dim=-1)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJR3djmek9TP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
